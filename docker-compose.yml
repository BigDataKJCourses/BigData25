services:
  metastore:
    image: postgres:11
    hostname: metastore
    environment:
      POSTGRES_PASSWORD: jupyter
    ports:
      - "5432:5432"
    volumes:
      - metastore:/var/lib/postgresql/data
      - ./metastore/ddl/init.sql:/docker-entrypoint-initdb.d/init.sql:ro
    restart: always
    networks:
      sparknet:
        ipv4_address: 172.28.1.1

  master:
    build:
      context: ./master
      dockerfile: Dockerfile
    image: hadoop-hive-spark-master:latest
    hostname: master
    depends_on:
      - metastore
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.2
      SPARK_LOCAL_HOSTNAME: master
      PYSPARK_PYTHON: /opt/venv/bin/python
      NAMEDIR: /opt/hadoop/dfs/name
      SPARK_LOGS_HDFS_PATH: /user/spark/logs
      SPARK_JARS_HDFS_PATH: /user/spark/jars
      HADOOP_CONF_DIR: /etc/hadoop/conf
      HIVE_CONF_DIR: /etc/hive/conf
      TEZ_CONF_DIR: /etc/tez/conf
    ports:
      - "4040:4040" # Spark Driver UI
      - "8020:8020" # HDFS NameNode RPC port
      - "8080:8080" # Spark Master UI
      - "8088:8088" # YARN ResourceManager UI
      - "9870:9870" # HDFS NameNode Web UI
      - "9999:9999" # Tez UI (static files)
      - "10000:10000" # HiveServer2 (Thrift JDBC/Beeline)
      - "10002:10002" # Hive UI (e.g. HiveView)
      - "8888:8888"  # JupyterLab
      - "8188:8188"  # YARN Timeline Server (ATS) UI
      - "18080:18080"  # Spark History Server UI
      - "19888:19888"  # MR JobHistoryServer UI
    volumes:
      - namenode:/opt/hadoop/dfs/name
      - ./work:/home/hadoop
      - ./conf/hadoop:/etc/hadoop/conf:ro
      - ./conf/hive:/etc/hive/conf:ro
      - ./conf/tez:/etc/tez/conf:ro
      - ./conf/spark:/etc/spark/conf:ro
      - ./master/entrypoint.sh:/usr/local/sbin/entrypoint.sh:ro
    restart: always
    networks:
      sparknet:
        ipv4_address: 172.28.1.2

  worker1:
    build:
      context: ./worker
      dockerfile: Dockerfile
    image: hadoop-hive-spark-worker:latest
    hostname: worker1
#    depends_on:
#      - master
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.3
      SPARK_LOCAL_HOSTNAME: worker1
      HADOOP_CONF_DIR: /etc/hadoop/conf
      TEZ_CONF_DIR: /etc/tez/conf
    ports:
      - "8042:8042"
      - "8081:8081"
      - "9864:9864"
    volumes:
      - datanode1:/opt/hadoop/dfs/data
      - ./conf/hadoop:/etc/hadoop/conf:ro
      - ./conf/hive:/etc/hive/conf:ro
      - ./conf/tez:/etc/tez/conf:ro
      - ./worker/entrypoint.sh:/usr/local/sbin/entrypoint.sh:ro
    restart: always
    networks:
      sparknet:
        ipv4_address: 172.28.1.3

  worker2:
    build:
      context: ./worker
      dockerfile: Dockerfile
    image: hadoop-hive-spark-worker:latest
    hostname: worker2
#    depends_on:
#      - master
    environment:
      SPARK_MASTER_HOST: 172.28.1.2
      SPARK_LOCAL_IP: 172.28.1.4
      SPARK_LOCAL_HOSTNAME: worker2
      HADOOP_CONF_DIR: /etc/hadoop/conf
      TEZ_CONF_DIR: /etc/tez/conf
    ports:
      - "8043:8042"
      - "8082:8081"
      - "9865:9864"
    volumes:
      - datanode2:/opt/hadoop/dfs/data
      - ./conf/hadoop:/etc/hadoop/conf:ro
      - ./conf/hive:/etc/hive/conf:ro
      - ./conf/tez:/etc/tez/conf:ro
      - ./worker/entrypoint.sh:/usr/local/sbin/entrypoint.sh:ro
    restart: always
    networks:
      sparknet:
        ipv4_address: 172.28.1.4

volumes:
  namenode:
  datanode1:
  datanode2:
  metastore:

networks:
  sparknet:
    ipam:
      driver: default
      config:
        - subnet: 172.28.0.0/16
